{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'\n",
    "file_name = 'sherlock.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(file_name, 'wb') as out_file:\n",
    "        data = response.read() # a `bytes` object\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '{*.txt}': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls {*.txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\r",
      "\r\n",
      "\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -2 sherlock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 1,33d sherlock.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF SHERLOCK HOLMES\r",
      "\r\n",
      "\r",
      "\r\n",
      "by\r",
      "\r\n",
      "\r",
      "\r\n",
      "SIR ARTHUR CONAN DOYLE\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE A\n"
     ]
    }
   ],
   "source": [
    "#let's the load data to RAM\n",
    "text = open(file_name, 'r', encoding='utf-8').read()  # note that I add an encoding='utf-8' parameter to preserve information\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is loaded as datatype: <class 'str'> and has 581204 characters in it\n"
     ]
    }
   ],
   "source": [
    "print(f'The file is loaded as datatype: {type(text)} and has {len(text)} characters in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n",
      "There are 85 unique characters, including both ASCII and Unicode character\n"
     ]
    }
   ],
   "source": [
    "# how many unique characters do we see? \n",
    "# For reference, ASCII has 127 characters in it - so we expect this to have at most 127 characters\n",
    "unique_chars = list(set(text))\n",
    "unique_chars.sort()\n",
    "print(unique_chars)\n",
    "print(f'There are {len(unique_chars)} unique characters, including both ASCII and Unicode character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "### Split by Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107431\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler.', 'All', 'emotions,', 'and', 'that', 'one', 'particularly,', 'were', 'abhorrent', 'to', 'his', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'He', 'was,', 'I', 'take', 'it,', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen,', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions,', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer.', 'They', 'were', 'admirable', 'things', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[90:200])  #start with the first chapeter, ignoring the index for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red-headed', 'woman', 'on', 'the', 'street']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at another example: \n",
    "'red-headed woman on the street'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by Word Extraction\n",
    "**Introducing Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words', 'words', '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\W+', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_alphanumeric = re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109111, 107431)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_alphanumeric), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOHEMIA', 'I', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', 'All', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'He', 'was', 'I', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'They', 'were', 'admirable']\n"
     ]
    }
   ],
   "source": [
    "print(words_alphanumeric[90:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isn', 't', 'he', 'coming', 'home', 'for', 'dinner', 'with', 'the', 'red', 'headed', 'girl', '']\n"
     ]
    }
   ],
   "source": [
    "words_break = re.split('\\W+', \"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print(words_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 455 ms, sys: 108 ms, total: 563 ms\n",
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[whole, of, her, sex, ., It, was, not, that, he, felt, \n",
      ", any, emotion, akin, to, love, for, Irene, Adler, ., All, emotions, ,, and, that, \n",
      ", one, particularly, ,, were, abhorrent, to, his, cold, ,, precise, but, \n",
      ", admirably, balanced, mind, ., He, was, ,, I, take, it, ,]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc)[150:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, spaCy tokenizes all *punctuations and words* and returned those as individual tokens as well. Let's try the example which we didn't like earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Is, n't, he, coming, home, for, dinner, with, the, red, -, headed, girl, ?]\n"
     ]
    }
   ],
   "source": [
    "words = nlp(\"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print([token for token in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Adventure of the Copper Beeches\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ", ADVENTURE I. A SCANDAL IN BOHEMIA\n",
      "\n",
      ", I.\n",
      "\n",
      ", To Sherlock Holmes she is always THE woman., I have seldom heard\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences[13:18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STOP WORD REMOVAL & CASE CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy has already marked each token as a stop word or not and stored it in `is_stop` attribute of each token. This makes it very handy for text cleaning. Let's take a quick look: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = \"the AI/AGI uprising cannot happen without the progress of NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, False, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, False, False)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE True False\n",
      "ADVENTURES False False\n",
      "OF True False\n",
      "SHERLOCK False False\n",
      "HOLMES False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:5]:\n",
    "    print(token, token.is_stop, token.is_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lower = text.lower()  # native python function\n",
    "doc_lower = nlp(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the True\n",
      "adventures False\n",
      "of True\n",
      "sherlock False\n",
      "holmes False\n"
     ]
    }
   ],
   "source": [
    "for token in doc_lower[:5]:\n",
    "    print(token, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spaCy has a dictionary of 312 stop words'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "f'spaCy has a dictionary of {len(list(STOP_WORDS))} stop words'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_stop_words = [\"NLP\", \"Processing\", \"AGI\"]\n",
    "for word in domain_stop_words:\n",
    "    STOP_WORDS.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(the, True, False),\n",
       " (AI, False, False),\n",
       " (/, False, True),\n",
       " (AGI, False, False),\n",
       " (uprising, False, False),\n",
       " (can, True, False),\n",
       " (not, True, False),\n",
       " (happen, False, False),\n",
       " (without, True, False),\n",
       " (the, True, False),\n",
       " (progress, False, False),\n",
       " (of, True, False),\n",
       " (NLP, False, False)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.is_stop, token.is_punct) for token in nlp(sentence_example)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', 'AGI', 'uprising', 'happen', 'progress', 'NLP']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop and not token.is_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI', '/', 'AGI', 'uprising', 'happen', 'progress', 'NLP']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(token) for token in nlp(sentence_example) if not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy for Lemmatization\n",
    "**spaCy only supports lemmatization** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An underscore at end, such as `lemma_` tells spaCy we are looking for something which is human readable. spaCy stores the internal hash or identifier which spaCy stores in `token.lemma`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Their, '-PRON-', 561228191312463089, 'DET'),\n",
       " (Apples, 'Apples', 9297668116247400838, 'PROPN'),\n",
       " (&, '&', 15473034735919704609, 'CCONJ'),\n",
       " (Banana, 'Banana', 7617506991971869807, 'PROPN'),\n",
       " (fruit, 'fruit', 17674554054627885835, 'NOUN'),\n",
       " (salads, 'salad', 16382906660984395826, 'NOUN'),\n",
       " (are, 'be', 10382539506755952630, 'VERB'),\n",
       " (amazing, 'amazing', 12968186374132960503, 'ADJ'),\n",
       " (., '.', 12646065887601541794, 'PUNCT'),\n",
       " (Would, 'Would', 10299253490465169573, 'VERB'),\n",
       " (you, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (like, 'like', 18194338103975822726, 'VERB'),\n",
       " (meeting, 'meet', 6880656908171229526, 'VERB'),\n",
       " (me, '-PRON-', 561228191312463089, 'PRON'),\n",
       " (at, 'at', 11667289587015813222, 'ADP'),\n",
       " (the, 'the', 7425985699627899538, 'DET'),\n",
       " (cafe, 'cafe', 10569699879655997926, 'NOUN'),\n",
       " (?, '?', 8205403955989537350, 'PUNCT')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_sentence_example = \"Their Apples & Banana fruit salads are amazing. Would you like meeting me at the cafe?\"\n",
    "[(token, token.lemma_, token.lemma, token.pos_ ) for token in nlp(lemma_sentence_example)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
