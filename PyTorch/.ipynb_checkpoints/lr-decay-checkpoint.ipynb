{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率衰减\n",
    "在 pytorch 中学习率衰减非常方便，使用 `torch.optim.lr_scheduler`，更多的信息可以直接查看[文档](http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate)\n",
    "\n",
    "但是我推荐大家使用下面这种方式来做学习率衰减，更加直观，下面我们直接举例子来说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.743150Z",
     "start_time": "2019-06-14T15:41:41.692933Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import CIFAR10\n",
    "from utils import resnet\n",
    "from torchvision import transforms as tfs\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.871999Z",
     "start_time": "2019-06-14T15:41:42.748718Z"
    }
   },
   "outputs": [],
   "source": [
    "net = resnet(3, 10)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们定义好了模型和优化器，可以通过 `optimizer.param_groups` 来得到所有的参数组和其对应的属性，参数组是什么意思呢？就是我们可以将模型的参数分成几个组，每个组定义一个学习率，这里比较复杂，一般来讲如果不做特别修改，就只有一个参数组\n",
    "\n",
    "这个参数组是一个字典，里面有很多属性，比如学习率，权重衰减等等，我们可以访问以下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.881924Z",
     "start_time": "2019-06-14T15:41:42.874050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.01\n",
      "weight decay: 0.0001\n"
     ]
    }
   ],
   "source": [
    "print('learning rate: {}'.format(optimizer.param_groups[0]['lr']))\n",
    "print('weight decay: {}'.format(optimizer.param_groups[0]['weight_decay']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们可以通过修改这个属性来改变我们训练过程中的学习率，非常简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.887882Z",
     "start_time": "2019-06-14T15:41:42.885163Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr'] = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了防止有多个参数组，我们可以使用一个循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.893980Z",
     "start_time": "2019-06-14T15:41:42.890887Z"
    }
   },
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法就是这样，非常简单，我们可以在任意的位置改变我们的学习率\n",
    "\n",
    "下面我们具体来看看学习率衰减的好处"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.899113Z",
     "start_time": "2019-06-14T15:41:42.895817Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.980418Z",
     "start_time": "2019-06-14T15:41:42.901086Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7eb079092d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             raise RuntimeError('Dataset not found or corrupted.' +\n\u001b[0;32m---> 70\u001b[0;31m                                ' You can use download=True to download it')\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "# 使用数据增强\n",
    "def train_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        tfs.Resize(120),\n",
    "        tfs.RandomHorizontalFlip(),\n",
    "        tfs.RandomCrop(96),\n",
    "        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "def test_tf(x):\n",
    "    im_aug = tfs.Compose([\n",
    "        tfs.Resize(96),\n",
    "        tfs.ToTensor(),\n",
    "        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    x = im_aug(x)\n",
    "    return x\n",
    "\n",
    "train_set = CIFAR10('./data', train=True, transform=train_tf, download=True)\n",
    "train_data = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=4)\n",
    "valid_set = CIFAR10('./data', train=False, transform=test_tf, download=True)\n",
    "valid_data = torch.utils.data.DataLoader(valid_set, batch_size=256, shuffle=False, num_workers=4)\n",
    "\n",
    "net = resnet(3, 10)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.984584Z",
     "start_time": "2019-06-14T15:41:41.791Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()\n",
    "prev_time = datetime.now()\n",
    "for epoch in range(30):\n",
    "    if epoch == 20:\n",
    "        set_learning_rate(optimizer, 0.01) # 80 次修改学习率为 0.01\n",
    "    train_loss = 0\n",
    "    net = net.train()\n",
    "    for im, label in train_data:\n",
    "        if torch.cuda.is_available():\n",
    "            im = Variable(im.cuda())  # (bs, 3, h, w)\n",
    "            label = Variable(label.cuda())  # (bs, h, w)\n",
    "        else:\n",
    "            im = Variable(im)\n",
    "            label = Variable(label)\n",
    "        # forward\n",
    "        output = net(im)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "    cur_time = datetime.now()\n",
    "    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "    m, s = divmod(remainder, 60)\n",
    "    time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    net = net.eval()\n",
    "    for im, label in valid_data:\n",
    "        if torch.cuda.is_available():\n",
    "            im = Variable(im.cuda(), volatile=True)\n",
    "            label = Variable(label.cuda(), volatile=True)\n",
    "        else:\n",
    "            im = Variable(im, volatile=True)\n",
    "            label = Variable(label, volatile=True)\n",
    "        output = net(im)\n",
    "        loss = criterion(output, label)\n",
    "        valid_loss += loss.data[0]\n",
    "    epoch_str = (\n",
    "        \"Epoch %d. Train Loss: %f, Valid Loss: %f, \"\n",
    "        % (epoch, train_loss / len(train_data), valid_loss / len(valid_data)))\n",
    "    prev_time = cur_time\n",
    "    \n",
    "    train_losses.append(train_loss / len(train_data))\n",
    "    valid_losses.append(valid_loss / len(valid_data))\n",
    "    print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们画出 loss 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.985611Z",
     "start_time": "2019-06-14T15:41:41.803Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-14T15:41:42.986578Z",
     "start_time": "2019-06-14T15:41:41.807Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(valid_losses, label='valid')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们只训练了 30 次，在 20 次的时候进行了学习率衰减，可以看 loss 曲线在 20 次的时候不管是 train loss 还是 valid loss，都有了一个陡降。\n",
    "\n",
    "当然这里我们只是作为举例，在实际应用中，做学习率衰减之前应该经过充分的训练，比如训练 80 次或者 100 次，然后再做学习率衰减得到更好的结果，有的时候甚至需要做多次学习率衰减"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
