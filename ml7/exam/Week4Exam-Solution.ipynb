{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营七期第四周((Hadoop&Spark)考试-参考答案\n",
    "#### 参考答案说明:\n",
    "\n",
    "- 来源：来自于网络搜索，笔记整理，当期及往期优秀同学试卷等途径\n",
    "- 使用：该答案仅供参考，非唯一固定答案，欢迎同学批评，指正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题10分，共计40分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#### hadoop是一个适合大数据的分布式存储和计算的平台。它具有如下优点：   \n",
    "   - 低成本:hadoop本身是运行在普通PC服务器组成的集群中进行大数据的分发及处理工作的，这些服务器集群是可以支持数千个节点的。   \n",
    "   - 高效性:这也是hadoop的核心竞争优势所在，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。   \n",
    "   - 可靠性:通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。   \n",
    "   - 扩展性:hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。   \n",
    "   - 容错性:hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配   \n",
    "\n",
    "\n",
    "#### Hadoop的局限性和不足：\n",
    "- 抽象层次低，需要手动编写mapper和reducer逻辑，使用上复杂\n",
    "- 只提供Map和reduce两个操作，表达力欠缺\n",
    "- 处理逻辑隐藏在代码细节中，没有整理逻辑\n",
    "- 中间结果也存放在HDFS上，IO和通信开销大\n",
    "- 时延高，只适用batch数据处理，对于交互式数据处理和实时数据处理的支持不够\n",
    "- 对于迭代式数据处理性能比较差\n",
    "<br>\n",
    "\n",
    "#### Spark相比hadoop的优点有：\n",
    "-  丰富的API，而后者只有map和reduce。\n",
    "-  中间结果不存磁盘，而后者需要把中间结果写磁盘。\n",
    "-  线程池模型减少task启动开销，而后者任务调度和启动开销很大。\n",
    "-  能充分利用内存速度快优势，减少IO操作，而后者不能。\n",
    "-  可以避免排序操作，而hadoop中map和reduce都需要排序。\n",
    "-  适合迭代计算（机器学习算法），而后者不适合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop包括分布式文件系统hdfs,和分布式计算框架，MapReduce是用于数据处理的一种编程模型,是hadoop的核心组件之一，可以通过mapreduce很容易在hadoop平台上进行分布式的计算编程。 \n",
    "\n",
    "#### MapRedeuce其处理过程主要分为两个步骤：<br>\n",
    "\n",
    "1、映射(Mapping)函数以Key/Value数据对作为输入，将输入数据经过业务逻辑计算产生若干仍旧以Key/Value形式表达的中间数。 MapReduce计算框架会自动将中间结果中具有相同Key值的记录聚合在一起，并将数据传送给Reduce函数内定义好的处理逻辑作为其输入值。\n",
    "\n",
    "2、聚合(Reducing)函数接收到Map阶段传过来的某个Key值及其对应的若干Value值等中间数据，函数逻辑对这个Key对应的Value内容进行处理，一般是对其进行累加、过滤、转换等操作，生成Key/Value形式的结果，这就是最终的业务计算结果。\n",
    "\n",
    "<img src=\"https://img.xiaoxiaomo.com/blog%2Fimg%2F20160412199999.png\" /> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "RDD(Resilient Distributed Datasets),弹性分布式数据集是一个容错的、可以被并行操作的元素集合弹性分布数据集。是Spark的核心，也是整个Spark的架构基础。Spark是以RDD概念为中心运行的。\n",
    "\n",
    "RDD的一大特性是分布式存储，分布式存储在最大的好处是可以让数据在不同工作节点并行存储，以便在需要数据时并行运算。弹性指其在节点存储时，既可以使用内存，也可已使用外存，为使用者进行大数据处理提供方便\n",
    "\n",
    "它的特性可以总结如下：\n",
    "- 它是不变的数据结构存储\n",
    "- 只读特性，维护DAG以便通过重新计算获得容错性\n",
    "- 它是支持跨集群的分布式数据结构\n",
    "- 可以根据数据记录的key对结构进行分区\n",
    "- 提供了粗粒度的操作，且这些操作都支持分区\n",
    "- 它将数据存储在内存中，从而提供了低延迟性\n",
    "\n",
    "常用的transformation操作：  \n",
    "* **`map(func)` 对调用map的RDD数据集中的每个element都使用func，然后返回一个新的RDD,这个返回的数据集是分布式的数据集 **\n",
    "* **`filter(func)` 对调用filter的RDD数据集中的每个元素都使用func，然后返回一个包含使func为true的元素构成的RDD  **\n",
    "* **`flatMap(func)` 和map差不多，但是flatMap生成的是多个结果,返回值是一个Seq(一个List)**\n",
    "* **`sample(withReplacement, fraction, seed)` 从RDD中的item中采样一部分出来，有放回或者无放回**\n",
    "* **`union(otherDataset)` 返回一个新的dataset，包含源dataset和给定dataset的元素的集合**  \n",
    "* **`distinct([numTasks]))` 对RDD中的item去重**\n",
    "* **`groupByKey([numTasks])`: 返回(K,Seq[V])，也就是hadoop中reduce函数接受的key-valuelist  **\n",
    "* **`reduceByKey(func, [numTasks])`: 就是用一个给定的reduce func再作用在groupByKey产生的(K,Seq[V]),比如求和，求平均数  **\n",
    "* **`sortByKey([ascending], [numTasks])` 按照key来进行排序，是升序还是降序，ascending是boolean类型  **  \n",
    "* **`join(otherDataset, [numTasks])` 当有两个KV的dataset(K,V)和(K,W)，返回的是(K,(V,W))的dataset,numTasks为并发的任务数 ** \n",
    "* **`cartesian(otherDataset)` 笛卡尔积就是m*n  **  \n",
    "* **`intersection(otherDataset)`: 交集**\n",
    "* **`substract(otherDataset)`: 差集**\n",
    "* **`sortBy(keyfunc, ascending=True, numPartitions=None)`: Sorts this RDD by the given keyfunc**\n",
    "\n",
    "常用的action操作：\n",
    "* **`reduce(func)`: 对RDD中的items做聚合**  \n",
    "* **`collect()`: 计算所有的items并返回所有的结果到driver端，接着 `collect()`会以Python list的形式返回结果**  \n",
    "* **`count()`: 返回的是dataset中的element的个数 **  \n",
    "* **`first()`: 和上面是类似的，不过只返回第1个item**  \n",
    "* **`take(n)`: 类似，但是返回n个item **  \n",
    "* **`top(n)`: 返回头n个items，按照自然结果排序**  \n",
    "* **`countByKey()`:  返回的是key对应的个数的一个map，作用于一个RDD  **\n",
    "* **`foreach()`: 对dataset中的每个元素都使用func **\n",
    "* **`takeSample()`: 指定采样个数，返回相应的数目 **\n",
    "* **`saveAsTextFile(path)`: 把dataset写到一个text file中，或者hdfs，或者hdfs支持的文件系统中，spark把每条记录都转换为一行记录，然后写到file中  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，如对该库不了解，可GOOGLE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 1、数据探索\n",
    " > 拿到数据以后，一般不会急于创建模型、训练模型，在这之前，需要对数据、对需求或机器学习的目标进行分析，尤其对数据进行一些必要的探索，如了解数据的大致结构、数据量、各特征的统计信息、整个数据质量情况、数据的分布情况等。为了更好体现数据分布情况，数据可视化是一个不错方法。\n",
    "* 2、预处理数据  \n",
    " > 通过对数据探索后，可能发现不少问题：如存在缺失数据、数据不规范、数据分布不均衡、存在奇异数据、有很多非数值数据、存在很多无关或不重要的数据等等。这些问题的存在直接影响数据质量，为此，数据预处理工作应该就是接下来的重点工作，数据预处理是机器学习过程中必不可少的重要步骤，特别是在生产环境中的机器学习，数据往往是原始、为加工和处理过，数据预处理常常占据整个机器学习过程的大部分时间。数据预处理过程中，一般包括数据清理、数据转换、规范数据、特征选择等等工作。\n",
    "* 3、训练模型  \n",
    " > 在模型选择时，一般不存在某种对任何情况都表现很好的算法（这种现象又称为没有免费的午餐）。因此在实际选择时，一般会选用几种不同方法来训练模型，然后比较它们的性能，从中选择最优的这个。当然，在比较不同模型之前，我们需要先确认衡量性能的指标，对分类问题常用的是准确率或ROC曲线，对回归连续性目标值问题一般采用误差来评估。训练模型前，一般会把数据集分为训练集和测试集，或对训练集再细分为训练集和验证集，从而对模型的泛化能力进行评估。\n",
    "* 4、评估模型与优化模型\n",
    " > 使用训练数据构建模型后，通常使用测试数据对模型进行测试，测试模型对新数据的测试。如果我们对模型的测试结果满意，就可以用此模型对以后的进行预测；如果我们测试结果不满意，我们可以优化模型，优化的方法很多，其中网格搜索参数是一种有效方法，当然我们也可以采用手工调节参数等方法。如果出现过拟合，尤其是回归类问题，我们可以考虑正则化方法来降低模型的泛化误差。\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计60分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(25分，每个小题5分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "\n",
    "1. 在hdfs根目录下新建/sxy-new文件夹\n",
    "- 把本地文件test.txt test2.txt放入该文件夹\n",
    "- 从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)\n",
    "- 从hdfs上取下/sxy-new中所有内容，并合成一个本地文件\n",
    "- 把/sxy-new拷贝到/tmp下后，删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -put test.txt test2.txt /sxy-new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -get /sxy-new/old.txt old_from_hadoop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-new merge.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -cp /sxy-new /temp, hadoop fs -rmr /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(45分)\n",
    "在服务器上执行pyspark命令可以启动，请把data.csv文件放置在服务器上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0.环境准备与启动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "conf=SparkConf().setAppName(\"miniProject\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据到raw_content这个RDD之中\n",
    "cwd = os.getcwd()\n",
    "s\n",
    "raw_content = sc.textFile(\"file://\"+cwd+\"/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过命令统计raw_content中的记录条数\n",
    "raw_content.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从raw_content这个RDD中取出前5条记录输出\n",
    "raw_content.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从raw_content中采样出3条记录输出\n",
    "raw_content.takeSample(False,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "# tip：参考map函数的用法\n",
    "content_rdd=raw_content.map(lambda x:x.replace('\"','')).map(lambda x:x.split(','))\n",
    "content_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "# tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "import re\n",
    "regexp = r'[\\s#,%]'\n",
    "reg = re.compile(regexp)\n",
    "def remove_symbol(s):\n",
    "    s = reg.sub('-',s)\n",
    "    return s.split('-')\n",
    "    \n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "text_rdd = sc.parallelize(text)\n",
    "text_rdd = text_rdd.flatMap(lambda s:remove_symbol(s))\n",
    "print(text_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "# tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "package_rdd = content_rdd.map(lambda s:s.split(',')[6]).filter(lambda s:s!='package').cache()\n",
    "package_count_rdd = package_rdd.map(lambda s:(s,1)).reduceByKey(lambda x,y:x+y)\n",
    "package_count_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出数量最多的10个package和出现的频次\n",
    "# tip：注意sortbykey的使用\n",
    "package_count_sort_rdd = package_count_rdd.map(lambda t:(t[1],t[0])).sortByKey(ascending=False).cache()\n",
    "package_count_sort_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 取出top5的package对应的数据记录\n",
    "# tip：注意filter的使用\n",
    "\n",
    "# 先取出top5 package对应的名字\n",
    "top5_package = [tup[1] for tup in package_count_sort_rdd.take(5)]\n",
    "top5_package_rdd = content_rdd.filter(lambda s:s.split(',')[6] in top5_package).cache()\n",
    "top5_package_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
