{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营第五周(机器学习基础)考试-参考答案\n",
    "#### 参考答案说明:\n",
    "\n",
    "- 来源：来自于网络搜索，笔记整理，当期及往期优秀同学试卷等途径\n",
    "- 使用：该答案仅供参考，非唯一固定答案，欢迎同学批评，指正。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入空间：所有输入可能的取值的集合\n",
    "- 输出空间：所有输出可能的取值的集合\n",
    "- 特征空间：每个具体的输入数据也叫一个实例通常由特征向量表示.所有特征向量存在的空间\n",
    "- 假设空间：由输入空间到输出空间映射的集合就是假设空间.假设空间的确定意味着学习范围的确定. \n",
    "\n",
    "- 参数空间：\n",
    "    - 非概率模型中，使得输入空间$X$中的变量$x$在通过假设空间$\\mathcal{F}$中的函数$f$后得到输出空间$Y$中的$y$成立的函数f的所有参数值（向量）组成的空间\n",
    "    - 概率模型中，使得输出空间$Y$中的变量$y$，在满足输入空间$X$中的变量$x$的条件下，使得$P(y|x)$成立的所有概率模型的参数向量组成的空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失函数：用来度量预测值$f(X)$与真实值$Y$相差的错误程度的函数，即$f(X)$和$Y$的非负实值函数，通常记作:$L(Y,f(X))$   \n",
    "\n",
    "\n",
    "- 常用的损失函数：\n",
    "    - 0-1损失函数：\n",
    "    $$L(Y, f(X)) = \\left\\{\\begin{aligned} & 1, Y \\neq f(X) \\\\ & 0, Y = f(X)\\end{aligned}\\right.$$ \n",
    "    - 平方损失函数\n",
    "    $$ L(Y, f(X)) = ( Y - f(X))^{2}$$ \n",
    "    - 绝对值损失函数 \n",
    "    $$ L(Y, f(X)) = |Y - f(X)|$$ \n",
    "    - 对数损失函数\n",
    "    $$ L(Y, P(Y|X)) = - \\log P(Y|X) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经验风险最小化的公式定义为：\n",
    "$$R_{emp}(f)=\\frac {1} {N} \\sum_{i=1}^N L(y_i,f(x_i)) $$\n",
    "\n",
    "结构风险最小化的公式定义为：\n",
    "$$R_{srm}(f)=\\frac {1} {N} \\sum_{i=1}^N L(y_i,f(x_i)) + \\lambda J(f) $$\n",
    "\n",
    "可以看到，结构风险在经验风险的基础上加上了表示模型复杂度的正则项或惩罚项，防止在数据样本小或者模型能力强的情况下出现过拟合。\n",
    "$J(f)$为模型的复杂度，模型 $f$ 越复杂，$J(f)$越大。$\\lambda>=0$是权衡经验风险和模型复杂度的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成模型\n",
    "    - 概念：由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型\n",
    "    - 特点：\n",
    "        - 可以还原出概率分布$P(X,Y)$\n",
    "        - 学习收敛速度更快\n",
    "        - 存在隐变量的时候仍然可以使用生成模型\n",
    "- 判别模型\n",
    "    - 概念：由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型\n",
    "    - 特点：\n",
    "        - 直接学习的是条件概率$P(Y|X)$或决策函数$f(X)$，通常准确率更高\n",
    "        - 可以抽象、定义特征，并增加、减少特征，简化学习问题\n",
    "        \n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "  <th>对比</th>\n",
    "  <th align=\"left\">判别式模型</th>\n",
    "  <th align=\"left\">生成式模型</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody><tr>\n",
    "  <td>特点</td>\n",
    "  <td align=\"left\">寻找不同类别之间的最优分类面，反映异类数据之间的差异</td>\n",
    "  <td align=\"left\">以统计的角度表示数据的分布情况，能够反映同类数据本身的相似度</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>区别（假如输入特征x,类别标签y）</td>\n",
    "  <td align=\"left\">估计的是条件概率分布：P(y|x)</td>\n",
    "  <td align=\"left\">估计的是联合概率分布 P(x,y)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>联系</td>\n",
    "  <td align=\"left\">由判别式模型<strong>不能得到</strong>生成式模型</td>\n",
    "  <td align=\"left\">由生成式模型<strong>可以得到</strong>判别式模型（贝叶斯公式）</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>优势</td>\n",
    "  <td align=\"left\">(1)能清晰地分辨出多类或某一类与其他类之间的差异特征；（2）适用于较多类别的识别；（3）模型更简单</td>\n",
    "  <td align=\"left\">（1）研究单类问题比判别式模型更灵活；（2）模型可以通过增强学习得到；（3）能用于数据不完整的情况。</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>缺点</td>\n",
    "  <td align=\"left\">不能反映训练数据本身的特性；</td>\n",
    "  <td align=\"left\">学习和计算过程比较复杂</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>性能</td>\n",
    "  <td align=\"left\">较好（因为利用了训练数据的类别标识信息）</td>\n",
    "  <td align=\"left\">较差</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>常见模型举例</td>\n",
    "  <td align=\"left\">KNN，SVM，决策树，线性回归，LR，boosting，线性判别分析（LDA），条件随机场，感知机，传统神经网络</td>\n",
    "  <td align=\"left\">朴素贝叶斯，隐马尔科夫模型，高斯混合模型，限制玻尔兹曼机</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>主要应用场景</td>\n",
    "  <td align=\"left\">图像文本分类，时间序列预测</td>\n",
    "  <td align=\"left\">NLP，医疗诊断</td>\n",
    "</tr>\n",
    "</tbody></table>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "范数(norm)可以简单的理解为向量的长度或大小，或者向量到零点（坐标轴原点）的距离，或者相应的两个向量点之间的距离。常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。其有着非负性；齐次性；三角不等式三个特点。   \n",
    "范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则\n",
    "\n",
    "- L1范数：向量各元素绝对值之和\n",
    "    $$\\lVert x \\lVert _{1} = \\sum_{i}|x_{i}|$$\n",
    "- L2范数：向量各元素绝对值的平方和后再开方\n",
    "    $$\\lVert x\\lVert _{2} = \\sqrt{\\sum_{i}x_{i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 信息熵\n",
    "    - 概念：在信息论与概率统计中，熵是表示随机变量不确定性的度量\n",
    "    - 公式：\n",
    "        $$H(X) =  -\\sum_{i=1}^{n}P_{i}\\log P_{i}$$\n",
    "- 信息增益\n",
    "    - 概念：使用某特征进行信息划分集合，使用划分前后集合熵的差值（信息的不确定减少的程度）来衡量使用当前特征对于样本集合D划分效果的好坏。\n",
    "    - 公式：\n",
    "        $$g(D,A) = H(D) - H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯的公式：\n",
    "$$P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}$$\n",
    "其中$P(Y)$是先验概率，$P(Y|X)$是后验概率<br><br>\n",
    "\n",
    "\n",
    "理解贝叶斯思想：   \n",
    "**逆概思想**: 我们容易计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”,很明显就是M/(N+M)。而一个自然而然的问题是反过来：“如果我们事先并不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例作出什么样的推测”。   \n",
    "而贝叶斯公式就是为解决这样的‘逆概率问题’而诞生的。***贝叶斯公式就是在不完全信息下，对部分位置状态用用主观概率或统计得来的先验概率。***贝叶斯公式对诱发某结果的可能性（原因）进行概率推理，即所谓‘逆概率问题’\n",
    "\n",
    "它与频率派的区别：\n",
    "  - 频率派：认为概率是客观的，是频率的极限表现。认为数据都是在某个参数（注意，这时是固定看待的常数值）下产生的，虽然不知道参数值是什么，但其一定存在，可以通过大量重复实验而估得。极大似然和置信区间这样的方法，他们更关心的是有多大的把握能找到那个固定的参数   \n",
    "  \n",
    "  \n",
    "  - 贝叶斯派：认为频率是主观的，是信心的绝佳解释。与频率派相反，其认为参数的取值是一个随机变量，是一个有关于参数取值的概率分布，这也是贝叶斯派的核心理念，其分布即先验概率分布，通过不断获取新的数据，通过MAP不断的调整刷新参数取值发生的概率，最终得到结果（概率的后验分布）。其最关心的是能覆盖住参数取值的区间，而不是某一具体值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设对于输入x分类为1和0的概率分别为：\n",
    "$$P(y = 1|x;\\theta) = h_{\\theta}(x)$$\n",
    "$$P(y = 0|x;\\theta) = 1 - h_{\\theta}(x)$$\n",
    "\n",
    "则概率函数为：\n",
    "$$P(y|x;\\theta) = (h_{\\theta})^{y} \\cdot (1 - h_{\\theta})^{1-y}$$\n",
    "\n",
    "假设样本数据有m个，并且独立，则他们的联合分布可以由各边际分布的乘积表示，则有似然函数为：\n",
    "$$L(\\theta) = \\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta) = \\prod_{i=1}^{m}(h_{\\theta}(x^{(i)}))^{y^{(i)}} \\cdot (1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}}$$\n",
    "\n",
    "取对数似然函数：\n",
    "$$l(\\theta) = \\log (L(\\theta)) = \\sum_{i=1}^{m}[\\log ((h_{\\theta}(x^{(i)}))^{y^{(i)}}) + \\log ((1 - h_{\\theta}(x^{(i)}))^{1-y^{(i)}})]$$\n",
    "\n",
    "化简：\n",
    "$$l(\\theta) = \\log (L(\\theta)) = \\sum_{i=1}^{m}[ y^{(i)}\\log (h_{\\theta}(x^{(i)})) + (1-y^{(i)}) \\log ((1 - h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "为了求解最优的参数$\\theta$，则需要最大化上述的对数似然函数，而上述的对数似然函数是以和的形式展示的，取负号，并乘以一个常数项，问题则变为求最小值，转化如下：\n",
    "$$-\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log (h_{\\theta}(x^{(i)})) + (1-y^{(i)}) \\log ((1 - h_{\\theta}(x^{(i)}))]$$\n",
    "\n",
    "就是逻辑回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min-max标准化：   \n",
    "也称为极差法，这是对原始数据的一种线性变换，使原始数据映射到[0-1]之间。本质上是指将原始数据的最大值映射成1，是最大值归一化。归一化的依据非常简单，不同变量往往量纲不同，归一化可以消除量纲对最终结果的影响，使不同变量具有可比性。\n",
    "\n",
    "$${ X }^{ * }=\\frac{X_{i} - X_{min}}{X_{max} - X_{min}}$$   \n",
    "Z-Score(标准化)：   \n",
    "也称为标准化分数，这种方法根据原始数据的均值和标准差进行标准化，即均值为0，标准差为1,它表示的是原始值与均值之间差多少个标准差，是一个相对值，有去除量纲的作用。\n",
    "$${ X }^{ * }=\\frac{X_{i} - \\mu}{\\sigma}$$\n",
    "\n",
    "其中$\\mu$和$\\sigma$代表样本的均值和标准差，$X_{max}$为特征取值的最大值，$X_{min}$为特征取值的最小值\n",
    "\n",
    "\n",
    "两者相似点：   \n",
    "两种处理的本质都是对数据做线性变换，将数据压缩到[0,1]区间或者最大标准差之间的范围，消除特征量纲对模型训练的影响\n",
    "\n",
    "\n",
    "两者不同点：   \n",
    "1. 归一化的缩放仅跟最大和最小值相关；而标准化的缩放和每个点都有关系，通过均值和方差体现出来<br>\n",
    "2. 归一化的输出范围在0-1之间；而标准化的输出范围是与数据的标准差相关<br>\n",
    "\n",
    "\n",
    "为什么树型结构不需要做缩放?   \n",
    "因为数值缩放不影响分裂点位置,对树模型的结构不造成影响。按照特征值对数据进行排序，排序的顺序不变，那么分裂点就不会不同。\n",
    "对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。而对于归一化的数据，损失等高线是圆形，更少的迭代次数即可到达最优点。树模型不使用梯度下降，因为构建树模型相当于寻找最优分裂点，因此树模型是阶跃的，在阶跃点处不可导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随机森林(Random Forest):利用了随机采样，对数据样本和特征进行抽样，训练出多个树分类器，避免了每树对所有样本及所有特征的学习，从而增加了随机性，避免了过拟合\n",
    "    1. 对训练样本数据进行有放回的抽样，生成K棵分类回归树\n",
    "    2. 假设特征空间有n个特征，每棵树的节点处随机抽取m个特征(m < n)\n",
    "    3. 使每棵树最大限度生长，不做任何剪枝\n",
    "    4. 通过多棵树组成森林，分类结果按树分类器投票多少决定\n",
    "- Xgboost:\n",
    "    - 本质上还是GBDT算法：\n",
    "    - 相对于GBDT的改进：\n",
    "        - 使用了L1、L2正则化，防止过拟合\n",
    "        - 对代价函数一阶、二阶求导，使得收敛更快\n",
    "        - 树生长完全后从底部向上剪枝，防止了算法贪婪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
