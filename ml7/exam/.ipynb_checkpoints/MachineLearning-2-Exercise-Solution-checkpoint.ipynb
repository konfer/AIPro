{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## MachineLearning 第二课作业\n",
    "\n",
    "####  作业提交说明：\n",
    "- 位置：作业文件统一放置于/0.Teacher/Exercise/MachineLearning-2/下\n",
    "- 文件名：请先复制该notebook文件，并重新命名为(课程名)+(您姓名的全拼)，并按要求完成后保存\n",
    "- 时间：课程结束后的第二天前提交。\n",
    "- 注意：请勿抄袭，移动，修改，删除其他同学和原始空白的练习文件。\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 机器学习算法有计算型和规则型这两类的算法，请问这两类下各自的都有哪些算法，以及您对这个分类思路的理解？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算型：LR、SVM\n",
    "> 计算型算法是构建线性或非线性函数，根据这个函数对未知样本进行预测。\n",
    "\n",
    "- 规则型：树模型（决策树、随机森林、XGBOOST等）\n",
    "> 规则型算法根据某些规则将样本划分到不同的类中，然后根据众数确定样本的类别，均值确定回归问题样本的预测值。\n",
    "\n",
    "\n",
    "- 注意：   \n",
    "学习方式不同，计算型算法是通过计算四则运算，会产出一个得分或者概率，再去进行相应比对；规则性算法产出的是一些规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.同样是进行分类任务，LR与决策树在解决问题的思想上有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LR是先构造线性函数，函数由特征和未知参数构成，然后对线性函数值进行sigmoid变换，变换成0-1的概率值，接近1的为一类，接近0的为一类。\n",
    "- 决策树是树状结构，自顶向下，每次选择一个特征进行分类，进而将样本分到不同的子节点。相比LR，决策树是非线性的，而且决策树还可以解决回归问题，在同一个子节点的用节点样本均值去预测，LR只能解决分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.如何理解决策树算法的可解释性很强这个优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:32:18.613753Z",
     "start_time": "2019-03-13T07:32:18.609088Z"
    }
   },
   "source": [
    "- 该算法是根据规则不断的将样本拆分，最终产生很多叶子节点，每个叶子节点对应的用户特征都是满足相同规则的。很容易了解样本的情况。\n",
    "- 决策树中树模型的结构是可以输出的,便于我们理解划分的规则,可以看到各个节点\n",
    "- 决策树是 if-then的形式规则判断来做划分便于理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.为什么说决策树在进行非线性表达这个层面要优于LR？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "决策树在进行属性分裂将样本分类的过程中，其实就是进行非线性分隔，而LR本身是线性函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释决策树结构中的各个组成部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "根结点、内部结点、叶子结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.描述下树模型的生成过程是怎样的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自顶向下不断分裂，每次分裂都选择最佳划分属性，直到树不能再分。\n",
    "\n",
    "- 最佳划分属性选择\n",
    " > 自根至叶结点递归，在每个中间结点寻找最优划分属性。划分属性的方式：信息增益（ID3算法）、信息增益率（C4.5算法)、基尼系数（CART算法）。\n",
    "- 树的停止条件\n",
    " >1）当前结点包含的样本都属于同一类，纯度很高，无需划分   \n",
    " >2）当前结点，样本属性集为空或者样本在属性上的取值相同，无法划分   \n",
    " >3）当前结点包含的样本集合为空或样本量很少，无法划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.请列出熵的计算公式，并解释其意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ent(D)=-\\sum_{i=0}^{n}p_ilog(p_i)$\n",
    "\n",
    "熵表示的是纯度，熵越大纯度越低，当结点中样本都属于某一类，熵的值为0，此时纯度最高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.决策树是如何完成回归任务的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归任务的损失函数是均方差，结点自顶向下分裂，每次分裂选择使得损失函数最小的属性及划分方式，分裂后样本分到不同的子节点中，一般用均值作为该结点的预测值（使得损失函数达到最小的预测值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.ID3,C4.5,CART三种树模型的区别在哪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、最佳属性选择方式不同   \n",
    "  >  ID3 根据信息增益，C4.5根据信息增益率，CART根据基尼稀疏\n",
    "2、处理的问题   \n",
    "  >  ID3、C4.5主要 是处理分类问题，CART被用来解决回归或者分类问题\n",
    "3、分支个数   \n",
    "  >  ID3、C4.5可以生成多个分支，CART是二分叉树\n",
    "4、特征使用次数   \n",
    "  >  对于连续特征，这三个模型都可以多次使用特征进行结点分裂，对于分类特征，只有CART模型可以多次使用该特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 如何控制树模型的过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:27:50.976486Z",
     "start_time": "2019-03-13T07:27:50.970919Z"
    }
   },
   "source": [
    "1、增加样本量   \n",
    "2、控制模型复杂度，比如限制最大树深、限制最小叶子结点样本量、结点进行分裂的样本最小值   \n",
    "3、学习率   \n",
    "4、阈值限定，比如若信息增益小于某个值，停止增长   \n",
    "5、交叉验证，如果验证集上目标函数下降变缓慢或开始上升，则停止   \n",
    "6、剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.连续变量与离散变量在树模型建模型时如何处理的 ？缺失值呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 连续变量的处理\n",
    "> 将连续变量从左至右升序排列，然后从左至右扫描，每次选择相邻点的中点进行分割，将连续变量离散化\n",
    "- 离散变量的处理\n",
    "> 离散变量一般不需要处理，如果是在CART算法中，将离散变量onehotEncoder\n",
    "- 缺失值\n",
    "> 缺失值时修正属性分裂指标（如信息增益等），修正后分裂指标值=未缺失样本占比*未缺失样本的属性分裂指标值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12.GBDT与XGBOOST有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 降低loss的方法不同\n",
    " >  XGBOOST每一轮增量是朝着牛顿方向，即 -梯度方向/hessian，让目标函数取到最小值   \n",
    " >  GBDT每一轮增量是朝着负梯度方向，让目标函数取到最小值\n",
    " \n",
    "- 是否加正则化项\n",
    " > XGBOOST在模型中加了正则化项"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
